{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Gathering the Data\n",
    "The first step is to gather a large amount of data and to store it in a pandas dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import secrets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "user_agent = \"Subreddit-Predictor 0.1 by /u/IsThisATrollBot\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets.client_ID,\n",
    "    client_secret=secrets.client_secret,\n",
    "    password=secrets.password,\n",
    "    user_agent=user_agent,\n",
    "    username=secrets.username,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because pushshift is down, we are limited to the amount of data we can gather at a time. So we will choose posts from the 10 most popular subreddits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Start with a list of subreddits\n",
    "top_subreddits = ['announcements', 'funny', 'AskReddit', 'gaming', 'Awww', 'Music', 'pics', 'science', 'worldnews', 'videos', 'AmItheAsshole']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create an empty list to store the posts\n",
    "posts = []\n",
    "\n",
    "# Iterate through the subreddits and get the last 1000 posts from each\n",
    "for sub in top_subreddits:\n",
    "    subreddit_posts = reddit.subreddit(sub).new(limit=1000)\n",
    "    for post in subreddit_posts:\n",
    "        posts.append(post)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Create a list of dictionaries containing the data for each post\n",
    "data = [{'id': post.id, 'title': post.title, 'subreddit': post.subreddit.display_name} for post in posts]\n",
    "\n",
    "# Create a Pandas dataframe from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "test_titles = ['Redditors of Reddit. What is your favorite piece of Reddit history?', 'WIBTA if I stole my younger brothers lunch money?', 'check out this cool video I found', 'asdf', 'cats are dangerous', 'new study shows cats are dangerous', 'reddit cool aita']\n",
    "test_titles = pd.DataFrame({'title':test_titles})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [],
   "source": [
    "class Subreddit_Predictor:\n",
    "    def __init__(self):\n",
    "        self.raw_data = pd.DataFrame({'id':[], 'title':[], 'subreddit':[]})\n",
    "        self.subreddits = []\n",
    "        self.data = pd.DataFrame({'id':[], 'title':[], 'subreddit':[]})\n",
    "        self.Feature_Vectors = {}\n",
    "        self.Embedding = {}\n",
    "        self.Title_Vectorizers = {}\n",
    "\n",
    "    def add_data(self, df):\n",
    "        \"\"\"df is a pandas DataFrame with columns={'title':[], 'subreddit':[]}. It will be merged with the existing raw_data\"\"\"\n",
    "        self.raw_data = pd.concat([self.raw_data, df]).drop_duplicates(subset='id')\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Cleans the data in raw_data and updates self.data\"\"\"\n",
    "\n",
    "        df = self.raw_data\n",
    "\n",
    "        # Remove all non-alpha-numeric characters\n",
    "        df['title'] = df['title'].str.replace(r'[^a-zA-Z0-9 ]', '', regex = True)\n",
    "\n",
    "        # Make all the text lowercase\n",
    "        df['title'] = df['title'].str.lower()\n",
    "\n",
    "        # Remove empty rows\n",
    "        df['title'] = df['title'].str.strip()\n",
    "        filter = df['title'] == ''\n",
    "        df = df.drop(df[filter].index)\n",
    "\n",
    "        # Store it as\n",
    "        self.data = df\n",
    "\n",
    "        #update the subreddits attribute\n",
    "        self.subreddits = self.data['subreddit'].unique().tolist()\n",
    "\n",
    "    def ready_data(self, test_size = .2, seed = 42):\n",
    "        \"\"\"Splits and encodes the data. Saves is in X_train, Y_train, X_test, Y_test.\"\"\"\n",
    "\n",
    "        # Change the index\n",
    "        self.data = self.data.set_index('id')\n",
    "\n",
    "        # Encode the subreddits\n",
    "        self._le = LabelEncoder()\n",
    "        self.data['subreddit_num'] = self._le.fit_transform(self.data['subreddit'])\n",
    "\n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.data['title'], self._le.fit_transform(self.data['subreddit']), test_size=test_size, random_state = seed)\n",
    "\n",
    "    def add_title_vectorizer(self, title_vectorizer):\n",
    "        \"\"\"This is how we add a title_vectorizer to our collection\"\"\"\n",
    "        title_vectorizer.train(self.X_train)\n",
    "        self.Title_Vectorizers[title_vectorizer.featureName] = title_vectorizer\n",
    "        self.Feature_Vectors[title_vectorizer.featureName] = title_vectorizer.vectorize(self.X_train)\n",
    "\n",
    "\n",
    "    def generate_features(self, featureName):\n",
    "        \"\"\"Generates the features using the different methods we have created\"\"\"\n",
    "\n",
    "        if featureName == 'BoW':\n",
    "            self.Embedding['BoW'] = CountVectorizer()\n",
    "            self.Features['BoW'] = self.Embedding['BoW'].fit_transform(self.X_train)\n",
    "\n",
    "        if featureName == 'D2V':\n",
    "\n",
    "            # Create a list of TaggedDocument objects from the titles\n",
    "            X_train_tagged = self.X_train.tolist()\n",
    "            X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "            X_test_tagged = self.X_test.tolist()\n",
    "            X_test_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_test_tagged)]\n",
    "\n",
    "            model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "            model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "            # Train the model\n",
    "            model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "            # Get the vectorized titles from the doc2vec model\n",
    "            vectors = [model_dbow.infer_vector(title.split()) for title in X_train.tolist()]\n",
    "\n",
    "            # Add the vectors to the dataframe as a new column\n",
    "            df_new = pd.DataFrame({'title':X_train, 'vector': vectors})\n",
    "            df_new\n",
    "\n",
    "    def vectorize(self, featureName, x):\n",
    "        \"\"\"Turns a sentence or list of sentences into a feature vectors\"\"\"\n",
    "\n",
    "        if type(x) == str: return self.vectorize(featureName, [x])\n",
    "\n",
    "        else:\n",
    "            if featureName == 'BoW':\n",
    "                return self.Embedding['BoW'].transform(x).toarray()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "class Title_Vectorizer:\n",
    "    \"\"\"This class is to hold all of the Title Vectorizers, like Bag-of-Words and Doc2Vec. Each vectorizer is a specific object. The class methods all have the same input/output.\"\"\"\n",
    "    def __init__(self, featureName):\n",
    "        self.featureName = featureName\n",
    "        self.description = \"Description goes here\"\n",
    "\n",
    "    def train(self, X_train):\n",
    "        \"\"\"Inputs the training data. Creates the self.model\"\"\"\n",
    "\n",
    "        self.model = self._train(X_train)\n",
    "\n",
    "    def _train(self, X_train):\n",
    "        \"\"\"Just a place holder for the actual function\"\"\"\n",
    "        #pass\n",
    "\n",
    "    def vectorize(self, df_titles):\n",
    "        \"\"\"Given a data frame or series with only titles, will return a df of all of the features, indexed by id. The actual function will be added to each object.\"\"\"\n",
    "\n",
    "        return self._vectorize(df_titles, self.model)\n",
    "\n",
    "    def _vectorize(self, df_titles, model):\n",
    "        \"\"\"Just a place holder for the actual function.\"\"\"\n",
    "        #pass\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [],
   "source": [
    "BoW_model = Title_Vectorizer('BoW')\n",
    "\n",
    "def _BoW_vectorize(df_titles, model):\n",
    "    \"\"\"I think I need to drop every word that's not in the vocabulary.\"\"\"\n",
    "\n",
    "    if type(df_titles) == pd.core.frame.DataFrame:\n",
    "        titles = df_titles['title']\n",
    "    else:\n",
    "        titles = df_titles\n",
    "\n",
    "    vocab = model.vocabulary_\n",
    "\n",
    "    titles = titles.apply(lambda s: ' '.join(set(s.split()).intersection(vocab)))\n",
    "    temp = model.transform(titles)\n",
    "    temp = temp.toarray()\n",
    "    temp = pd.DataFrame(temp)\n",
    "    temp['id'] =df_titles.index\n",
    "    temp = temp.set_index('id')\n",
    "    return temp\n",
    "\n",
    "def _BoW_train(X_train):\n",
    "    model = CountVectorizer()\n",
    "    model.fit_transform(X_train)\n",
    "    return model\n",
    "\n",
    "BoW_model._vectorize = _BoW_vectorize\n",
    "BoW_model._train = _BoW_train\n",
    "\n",
    "BoW_model.train(obj.X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "BoW_model = Title_Vectorizer('BoW')\n",
    "BoW_model._vectorize = _BoW_vectorize\n",
    "BoW_model._train = _BoW_train\n",
    "#BoW_model.train(obj.X_train)\n",
    "#BoW_model.vectorize(obj.X_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [
    {
     "data": {
      "text/plain": "sklearn.feature_extraction.text.CountVectorizer"
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_BoW_train(obj.X_train)\n",
    "BoW_model._train = _BoW_train\n",
    "BoW_model.train(obj.X_train)\n",
    "type(BoW_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [
    {
     "data": {
      "text/plain": "NoneType"
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(BoW_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW_model.model.transform(list(test_titles['title'])).toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    I love\n",
      "1    I hate\n",
      "2    I like\n",
      "dtype: object\n",
      "0    I love\n",
      "1    I hate\n",
      "2    I like\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample pandas series\n",
    "s = pd.Series(['I love dogs', 'I hate cats', 'I like turtles'])\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = ['I', 'love', 'hate', 'like']\n",
    "\n",
    "# Remove words from the sentences that are not in the vocabulary\n",
    "filtered_s = s.apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "# Print the filtered series\n",
    "print(filtered_s)\n",
    "# Remove words from the sentences that are not in the vocabulary\n",
    "filtered_s = s.apply(lambda x: ' '.join(set(x.split()).intersection(vocab)))\n",
    "\n",
    "# Print the filtered series\n",
    "print(filtered_s)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = CountVectorizer()\n",
    "x.fit_transform(obj.X_train)\n",
    "vocab = x.vocabulary_\n",
    "'im' in vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               title\n0  Redditors of Reddit. What is your favorite pie...\n1  WIBTA if I stole my younger brothers lunch money?\n2                  check out this cool video I found\n3                                               asdf\n4                                 cats are dangerous\n5                 new study shows cats are dangerous\n6                                   reddit cool aita",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Redditors of Reddit. What is your favorite pie...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WIBTA if I stole my younger brothers lunch money?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>check out this cool video I found</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>asdf</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cats are dangerous</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>new study shows cats are dangerous</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>reddit cool aita</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_31196\\1401820855.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mBoW_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvectorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'title'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mtest_titles\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_31196\\1647834529.py\u001B[0m in \u001B[0;36mvectorize\u001B[1;34m(self, df_titles)\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[1;34m\"\"\"Given a data frame or series with only titles, will return a df of all of the features, indexed by id. The actual function will be added to each object.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_vectorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_vectorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_31196\\3159065385.py\u001B[0m in \u001B[0;36m_BoW_vectorize\u001B[1;34m(df_titles)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_BoW_vectorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;34m\"\"\"I think I need to drop every word that's not in the vocabulary.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mdf_titles\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_titles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m' '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mtemp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCountVectorizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtemp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_titles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   8846\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   8847\u001B[0m         )\n\u001B[1;32m-> 8848\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__finalize__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"apply\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   8849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   8850\u001B[0m     def applymap(\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    731\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_raw\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    732\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 733\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    734\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    735\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0magg\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    855\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    856\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 857\u001B[1;33m         \u001B[0mresults\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_series_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    858\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    859\u001B[0m         \u001B[1;31m# wrap results\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    871\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseries_gen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    872\u001B[0m                 \u001B[1;31m# ignore SettingWithCopy here in case the user mutates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 873\u001B[1;33m                 \u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    874\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mABCSeries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    875\u001B[0m                     \u001B[1;31m# If we have a view on v, we need to make a copy because\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_31196\\3159065385.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(s)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_BoW_vectorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;34m\"\"\"I think I need to drop every word that's not in the vocabulary.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mdf_titles\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_titles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m' '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mtemp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCountVectorizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtemp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_titles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5573\u001B[0m         ):\n\u001B[0;32m   5574\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5575\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5576\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5577\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "BoW_model.vectorize(pd.DataFrame({'title':test_titles}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6477 entries, zmng91 to zodvmd\n",
      "Columns: 13136 entries, 0 to 13135\n",
      "dtypes: int64(13136)\n",
      "memory usage: 649.4+ MB\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(x, obj.X_train.index).info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_31196\\3540296549.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'title'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'vector'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    634\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    635\u001B[0m             \u001B[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 636\u001B[1;33m             \u001B[0mmgr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdict_to_mgr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtyp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmanager\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    637\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMaskedArray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    638\u001B[0m             \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmrecords\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmrecords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001B[0m in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    500\u001B[0m         \u001B[1;31m# TODO: can we get rid of the dt64tz special case above?\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    501\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 502\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0marrays_to_mgr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtyp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtyp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconsolidate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    503\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001B[0m in \u001B[0;36marrays_to_mgr\u001B[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[0;32m    118\u001B[0m         \u001B[1;31m# figure out the index, if necessary\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 120\u001B[1;33m             \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_extract_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    121\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    122\u001B[0m             \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mensure_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001B[0m in \u001B[0;36m_extract_index\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    659\u001B[0m                 \u001B[0mraw_lengths\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    660\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mval\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 661\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Per-column arrays must each be 1-dimensional\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    662\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    663\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mindexes\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mraw_lengths\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "pd.DataFrame({'title':obj.X_train, 'vector': x})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "obj = Subreddit_Predictor()\n",
    "obj.add_data(df)\n",
    "obj.clean_data()\n",
    "obj.ready_data(test_size=.3, seed=29)\n",
    "obj.add_title_vectorizer(BoW_model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [
    {
     "data": {
      "text/plain": "        0      1      2      3      4      5      6      7      8      9      \\\nid                                                                             \nzmng91      0      0      0      0      0      0      0      0      0      0   \nzrve0c      0      0      0      0      0      0      0      0      0      0   \nzppddb      0      0      0      0      0      0      0      0      0      0   \nz4m4c6      0      0      0      0      0      0      0      0      0      0   \nzeb9r7      0      0      0      0      0      0      0      0      0      0   \n...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \nzo36za      0      0      0      0      0      0      0      0      0      0   \nzog02f      0      0      0      0      0      0      0      0      0      0   \nzophno      0      0      0      0      0      0      0      0      0      0   \nz7sghp      0      0      0      0      0      0      0      0      0      0   \nzodvmd      0      0      0      0      0      0      0      0      0      0   \n\n        ...  13126  13127  13128  13129  13130  13131  13132  13133  13134  \\\nid      ...                                                                  \nzmng91  ...      0      0      0      0      0      0      0      0      0   \nzrve0c  ...      0      0      0      0      0      0      0      0      0   \nzppddb  ...      0      0      0      0      0      0      0      0      0   \nz4m4c6  ...      0      0      0      0      0      0      0      0      0   \nzeb9r7  ...      0      0      0      0      0      0      0      0      0   \n...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \nzo36za  ...      0      0      0      0      0      0      0      0      0   \nzog02f  ...      0      0      0      0      0      0      0      0      0   \nzophno  ...      0      0      0      0      0      0      0      0      0   \nz7sghp  ...      0      0      0      0      0      0      0      0      0   \nzodvmd  ...      0      0      0      0      0      0      0      0      0   \n\n        13135  \nid             \nzmng91      0  \nzrve0c      0  \nzppddb      0  \nz4m4c6      0  \nzeb9r7      0  \n...       ...  \nzo36za      0  \nzog02f      0  \nzophno      0  \nz7sghp      0  \nzodvmd      0  \n\n[6477 rows x 13136 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>13126</th>\n      <th>13127</th>\n      <th>13128</th>\n      <th>13129</th>\n      <th>13130</th>\n      <th>13131</th>\n      <th>13132</th>\n      <th>13133</th>\n      <th>13134</th>\n      <th>13135</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>zmng91</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>zrve0c</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>zppddb</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>z4m4c6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>zeb9r7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>zo36za</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>zog02f</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>zophno</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>z7sghp</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>zodvmd</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6477 rows × 13136 columns</p>\n</div>"
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.Feature_Vectors['BoW']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 2\n"
     ]
    }
   ],
   "source": [
    "def foo(x):\n",
    "    print ('hello',x)\n",
    "\n",
    "obj.fun = foo\n",
    "\n",
    "obj.fun(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "          id                                              title      subreddit\n0     t93ec3  This subreddit is closed for new posts and com...  announcements\n1     pg006s          COVID denialism and policy clarifications  announcements\n2     pbmy5y             Debate, dissent, and protest on Reddit  announcements\n3     nw2hs6           Sunsetting Secret Santa and Reddit Gifts  announcements\n4     mi01fg                                             Second  announcements\n...      ...                                                ...            ...\n9261  zq0n2b               WIBTA For Exposing My Dad to My Mom?  AmItheAsshole\n9262  zq0kzb  AITA for trying to rescue/take home/whatever a...  AmItheAsshole\n9263  zq0kv9  AITA for not wanting to gift hotel soaps for C...  AmItheAsshole\n9264  zq0k55          AITA for walking my dog on my own street?  AmItheAsshole\n9265  zq0i4t  WIBTA if I called my cat with a nickname rathe...  AmItheAsshole\n\n[9266 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>subreddit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t93ec3</td>\n      <td>This subreddit is closed for new posts and com...</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pg006s</td>\n      <td>COVID denialism and policy clarifications</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pbmy5y</td>\n      <td>Debate, dissent, and protest on Reddit</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nw2hs6</td>\n      <td>Sunsetting Secret Santa and Reddit Gifts</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mi01fg</td>\n      <td>Second</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9261</th>\n      <td>zq0n2b</td>\n      <td>WIBTA For Exposing My Dad to My Mom?</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>9262</th>\n      <td>zq0kzb</td>\n      <td>AITA for trying to rescue/take home/whatever a...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>9263</th>\n      <td>zq0kv9</td>\n      <td>AITA for not wanting to gift hotel soaps for C...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>9264</th>\n      <td>zq0k55</td>\n      <td>AITA for walking my dog on my own street?</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>9265</th>\n      <td>zq0i4t</td>\n      <td>WIBTA if I called my cat with a nickname rathe...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n  </tbody>\n</table>\n<p>9266 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the labels to numerical values\n",
    "le = LabelEncoder()\n",
    "df['subreddit_num'] = le.fit_transform(df['subreddit'])\n",
    "\n",
    "df = df.drop(columns=['subreddit'])\n",
    "\n",
    "#df['subreddit'] = le.inverse_transform(df['subreddit_num'])\n",
    "\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame({'id':['pg006s'], 'title':[a], 'subreddit':['announcements']}).set_index('id')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    title      subreddit\nid                                                                      \npg006s          COVID denialism and policy clarifications  announcements\npbmy5y             Debate, dissent, and protest on Reddit  announcements\nnw2hs6           Sunsetting Secret Santa and Reddit Gifts  announcements\nmi01fg                                             Second  announcements\nmcisdf  An update on the recent issues surrounding a R...  announcements\n...                                                   ...            ...\nzq0n2b               WIBTA For Exposing My Dad to My Mom?  AmItheAsshole\nzq0kzb  AITA for trying to rescue/take home/whatever a...  AmItheAsshole\nzq0kv9  AITA for not wanting to gift hotel soaps for C...  AmItheAsshole\nzq0k55          AITA for walking my dog on my own street?  AmItheAsshole\nzq0i4t  WIBTA if I called my cat with a nickname rathe...  AmItheAsshole\n\n[9160 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>subreddit</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pg006s</th>\n      <td>COVID denialism and policy clarifications</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>pbmy5y</th>\n      <td>Debate, dissent, and protest on Reddit</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>nw2hs6</th>\n      <td>Sunsetting Secret Santa and Reddit Gifts</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>mi01fg</th>\n      <td>Second</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>mcisdf</th>\n      <td>An update on the recent issues surrounding a R...</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>zq0n2b</th>\n      <td>WIBTA For Exposing My Dad to My Mom?</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0kzb</th>\n      <td>AITA for trying to rescue/take home/whatever a...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0kv9</th>\n      <td>AITA for not wanting to gift hotel soaps for C...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0k55</th>\n      <td>AITA for walking my dog on my own street?</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0i4t</th>\n      <td>WIBTA if I called my cat with a nickname rathe...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n  </tbody>\n</table>\n<p>9160 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_new, df]).drop_duplicates(keep = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    title      subreddit\nid                                                                      \nt93ec3  This subreddit is closed for new posts and com...  announcements\npg006s          COVID denialism and policy clarifications  announcements\npbmy5y             Debate, dissent, and protest on Reddit  announcements\nnw2hs6           Sunsetting Secret Santa and Reddit Gifts  announcements\nmi01fg                                             Second  announcements\n...                                                   ...            ...\nzq0n2b               WIBTA For Exposing My Dad to My Mom?  AmItheAsshole\nzq0kzb  AITA for trying to rescue/take home/whatever a...  AmItheAsshole\nzq0kv9  AITA for not wanting to gift hotel soaps for C...  AmItheAsshole\nzq0k55          AITA for walking my dog on my own street?  AmItheAsshole\nzq0i4t  WIBTA if I called my cat with a nickname rathe...  AmItheAsshole\n\n[9210 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>subreddit</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>t93ec3</th>\n      <td>This subreddit is closed for new posts and com...</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>pg006s</th>\n      <td>COVID denialism and policy clarifications</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>pbmy5y</th>\n      <td>Debate, dissent, and protest on Reddit</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>nw2hs6</th>\n      <td>Sunsetting Secret Santa and Reddit Gifts</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>mi01fg</th>\n      <td>Second</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>zq0n2b</th>\n      <td>WIBTA For Exposing My Dad to My Mom?</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0kzb</th>\n      <td>AITA for trying to rescue/take home/whatever a...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0kv9</th>\n      <td>AITA for not wanting to gift hotel soaps for C...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0k55</th>\n      <td>AITA for walking my dog on my own street?</td>\n      <td>AmItheAsshole</td>\n    </tr>\n    <tr>\n      <th>zq0i4t</th>\n      <td>WIBTA if I called my cat with a nickname rathe...</td>\n      <td>AmItheAsshole</td>\n    </tr>\n  </tbody>\n</table>\n<p>9210 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(keep = 'first')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "2  2  5  8\n",
      "4  3  6  9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 2, 3, 3], 'B': [4, 5, 5, 6, 6], 'C': [7, 8, 8, 9, 9]})\n",
    "\n",
    "# Find duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "# Print the duplicate rows\n",
    "print(duplicate_rows)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    title      subreddit\nid                                                                      \nc0gl6   We are aware that reddit appears hung over, an...  announcements\nzsrwxs                   What made you want to have kids?      AskReddit\nzspw41                    What do you want for Christmas?      AskReddit\nzsppnr                   What made you want to have kids?      AskReddit\nzspbbv                    What do you want for Christmas?      AskReddit\nzsp834                   What made you want to have kids?      AskReddit\nzso4w7                What is on your Christmas wishlist?      AskReddit\nzso42r  People who have their desserts before their ma...      AskReddit\nzshreq  does crashing and desabling gpu driver means t...         gaming\nzs6y9p        Kingdoms of Amalur: Re-Reckoning worth $12?         gaming\nzml7qc                                              Cutie           Awww\nytaqyj                                                cat           Awww\nxxyrri     In a world where you can be anything, be kind.           Awww\nxuxlzv                                               awww           Awww\nxuco6g                                               Awww           Awww\nxr6c8n                                              Awwww           Awww\nzsduwh  \"3DOG - goodbye girl <3\" Go check me out and g...          Music\nzqi1ll                            Cameron Coyle on TikTok          Music\nzq63z8  Terry Hall: lead singer of the Specials dies a...          Music\nzq19h8              W0lfgxng - terminator [hiphop] (2022)          Music\nzpq77e                                                  🎅          Music\nzpq641                                                  🎅          Music\nzsc4e8                           My 7 year old drew this.           pics\nzrhjbz                             View from my backyard.           pics\nzrgaqa  Iranians are preparing to celebrate Yalda Nigh...           pics\nzr1t8l                              Marker on canvas [OC]           pics\nzq5wao                              Marker on canvas [OC]           pics\nzq5nfg                         I took a picture of a Lake           pics\nzsstby  Official in Russian-controlled Ukraine region ...      worldnews\nzspaem  US says Russia's Wagner Group bought North Kor...      worldnews\nzso92e  U.S. military aid for Ukraine: Five key weapon...      worldnews\nzsjkye  Pope warns Vatican staff an 'elegant demon' lu...      worldnews\nzsh0lo  Kremlin-backed hackers targeted a “large” petr...      worldnews\nzrxeq9  Russian ammunition storage points in Kadiivka ...      worldnews\nzrteq8  Ukraine war: Russia not to blame for conflict ...      worldnews\nzrs1dg  'We will find you:' Russians hunt down Ukraini...      worldnews\nzrkqq8  'We will find you:' Russians hunt down Ukraini...      worldnews\nzrjwo8  Brexit rule that makes EU citizens reapply to ...      worldnews\nzreq86  Hearses queue at Beijing crematorium, even as ...      worldnews\nzr8cez  Kremlin-backed hackers targeted a “large” petr...      worldnews\nzqu5fe  More Iranians face possible execution as autho...      worldnews\nzq43ig  Ukraine to boost Belarus border defences as Pu...      worldnews\nzptekm  Pakistani Taliban overpower guards, seize poli...      worldnews\nzpkg3o  Thailand warship capsizes leaving 31 sailors m...      worldnews\nzo6yec  Brixton Academy: Woman dies after Asake concer...      worldnews\nzo2shg  Hundreds of tourists stranded in Machu Picchu ...      worldnews\nzny8f5  Iranian government accused of 'sham trials' an...      worldnews\nzns4lr  South Korea protests Japan's new security docu...      worldnews\nzna1tr  Russia launches another major missile attack o...      worldnews\nzmln5m  Dutch chip equipment maker ASML's CEO question...      worldnews\nzmj7kd  Ukrainian photographers Yevhen Maloletka and M...      worldnews\nzmhdkd  Irish soldier on UN duty killed in Lebanon attack      worldnews\nzmgocl  Home-grown supply operation outfits Ukraine's ...      worldnews\nzrybuv                This Place Rules (Official Trailer)         videos\nzqn96m  Alice In Chains - Again\" & \"God Am [Tiger Stad...         videos\nzntpi1            Investigating Logan Paul's Biggest Scam         videos",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>subreddit</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>c0gl6</th>\n      <td>We are aware that reddit appears hung over, an...</td>\n      <td>announcements</td>\n    </tr>\n    <tr>\n      <th>zsrwxs</th>\n      <td>What made you want to have kids?</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zspw41</th>\n      <td>What do you want for Christmas?</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zsppnr</th>\n      <td>What made you want to have kids?</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zspbbv</th>\n      <td>What do you want for Christmas?</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zsp834</th>\n      <td>What made you want to have kids?</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zso4w7</th>\n      <td>What is on your Christmas wishlist?</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zso42r</th>\n      <td>People who have their desserts before their ma...</td>\n      <td>AskReddit</td>\n    </tr>\n    <tr>\n      <th>zshreq</th>\n      <td>does crashing and desabling gpu driver means t...</td>\n      <td>gaming</td>\n    </tr>\n    <tr>\n      <th>zs6y9p</th>\n      <td>Kingdoms of Amalur: Re-Reckoning worth $12?</td>\n      <td>gaming</td>\n    </tr>\n    <tr>\n      <th>zml7qc</th>\n      <td>Cutie</td>\n      <td>Awww</td>\n    </tr>\n    <tr>\n      <th>ytaqyj</th>\n      <td>cat</td>\n      <td>Awww</td>\n    </tr>\n    <tr>\n      <th>xxyrri</th>\n      <td>In a world where you can be anything, be kind.</td>\n      <td>Awww</td>\n    </tr>\n    <tr>\n      <th>xuxlzv</th>\n      <td>awww</td>\n      <td>Awww</td>\n    </tr>\n    <tr>\n      <th>xuco6g</th>\n      <td>Awww</td>\n      <td>Awww</td>\n    </tr>\n    <tr>\n      <th>xr6c8n</th>\n      <td>Awwww</td>\n      <td>Awww</td>\n    </tr>\n    <tr>\n      <th>zsduwh</th>\n      <td>\"3DOG - goodbye girl &lt;3\" Go check me out and g...</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>zqi1ll</th>\n      <td>Cameron Coyle on TikTok</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>zq63z8</th>\n      <td>Terry Hall: lead singer of the Specials dies a...</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>zq19h8</th>\n      <td>W0lfgxng - terminator [hiphop] (2022)</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>zpq77e</th>\n      <td>🎅</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>zpq641</th>\n      <td>🎅</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>zsc4e8</th>\n      <td>My 7 year old drew this.</td>\n      <td>pics</td>\n    </tr>\n    <tr>\n      <th>zrhjbz</th>\n      <td>View from my backyard.</td>\n      <td>pics</td>\n    </tr>\n    <tr>\n      <th>zrgaqa</th>\n      <td>Iranians are preparing to celebrate Yalda Nigh...</td>\n      <td>pics</td>\n    </tr>\n    <tr>\n      <th>zr1t8l</th>\n      <td>Marker on canvas [OC]</td>\n      <td>pics</td>\n    </tr>\n    <tr>\n      <th>zq5wao</th>\n      <td>Marker on canvas [OC]</td>\n      <td>pics</td>\n    </tr>\n    <tr>\n      <th>zq5nfg</th>\n      <td>I took a picture of a Lake</td>\n      <td>pics</td>\n    </tr>\n    <tr>\n      <th>zsstby</th>\n      <td>Official in Russian-controlled Ukraine region ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zspaem</th>\n      <td>US says Russia's Wagner Group bought North Kor...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zso92e</th>\n      <td>U.S. military aid for Ukraine: Five key weapon...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zsjkye</th>\n      <td>Pope warns Vatican staff an 'elegant demon' lu...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zsh0lo</th>\n      <td>Kremlin-backed hackers targeted a “large” petr...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zrxeq9</th>\n      <td>Russian ammunition storage points in Kadiivka ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zrteq8</th>\n      <td>Ukraine war: Russia not to blame for conflict ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zrs1dg</th>\n      <td>'We will find you:' Russians hunt down Ukraini...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zrkqq8</th>\n      <td>'We will find you:' Russians hunt down Ukraini...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zrjwo8</th>\n      <td>Brexit rule that makes EU citizens reapply to ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zreq86</th>\n      <td>Hearses queue at Beijing crematorium, even as ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zr8cez</th>\n      <td>Kremlin-backed hackers targeted a “large” petr...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zqu5fe</th>\n      <td>More Iranians face possible execution as autho...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zq43ig</th>\n      <td>Ukraine to boost Belarus border defences as Pu...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zptekm</th>\n      <td>Pakistani Taliban overpower guards, seize poli...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zpkg3o</th>\n      <td>Thailand warship capsizes leaving 31 sailors m...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zo6yec</th>\n      <td>Brixton Academy: Woman dies after Asake concer...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zo2shg</th>\n      <td>Hundreds of tourists stranded in Machu Picchu ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zny8f5</th>\n      <td>Iranian government accused of 'sham trials' an...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zns4lr</th>\n      <td>South Korea protests Japan's new security docu...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zna1tr</th>\n      <td>Russia launches another major missile attack o...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zmln5m</th>\n      <td>Dutch chip equipment maker ASML's CEO question...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zmj7kd</th>\n      <td>Ukrainian photographers Yevhen Maloletka and M...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zmhdkd</th>\n      <td>Irish soldier on UN duty killed in Lebanon attack</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zmgocl</th>\n      <td>Home-grown supply operation outfits Ukraine's ...</td>\n      <td>worldnews</td>\n    </tr>\n    <tr>\n      <th>zrybuv</th>\n      <td>This Place Rules (Official Trailer)</td>\n      <td>videos</td>\n    </tr>\n    <tr>\n      <th>zqn96m</th>\n      <td>Alice In Chains - Again\" &amp; \"God Am [Tiger Stad...</td>\n      <td>videos</td>\n    </tr>\n    <tr>\n      <th>zntpi1</th>\n      <td>Investigating Logan Paul's Biggest Scam</td>\n      <td>videos</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
