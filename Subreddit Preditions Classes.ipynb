{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Gathering the Data\n",
    "The first step is to gather a large amount of data and to store it in a pandas dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import secrets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.svm import SVC\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "user_agent = \"Subreddit-Predictor 0.1 by /u/IsThisATrollBot\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets.client_ID,\n",
    "    client_secret=secrets.client_secret,\n",
    "    password=secrets.password,\n",
    "    user_agent=user_agent,\n",
    "    username=secrets.username,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because pushshift is down, we are limited to the amount of data we can gather at a time. So we will choose posts from the 10 most popular subreddits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Start with a list of subreddits\n",
    "top_subreddits = ['announcements', 'funny', 'AskReddit', 'dataisbeautiful', 'Awww', 'datascience', 'pics', 'science', 'worldnews', 'videos', 'AmItheAsshole']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Create an empty list to store the posts\n",
    "posts = []\n",
    "\n",
    "# Iterate through the subreddits and get the last 1000 posts from each\n",
    "for sub in top_subreddits:\n",
    "    subreddit_posts = reddit.subreddit(sub).new(limit=1000)\n",
    "    for post in subreddit_posts:\n",
    "        posts.append(post)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create a list of dictionaries containing the data for each post\n",
    "data = [{'id': post.id, 'title': post.title, 'subreddit': post.subreddit.display_name} for post in posts]\n",
    "\n",
    "# Create a Pandas dataframe from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "test_titles = ['Redditors of Reddit. What is your favorite piece of Reddit history?', 'WIBTA if I stole my younger brothers lunch money?', 'check out this cool video I found', 'asdf', 'cats are dangerous', 'new study shows cats are dangerous', 'reddit cool aita']\n",
    "test_titles = pd.DataFrame({'title':test_titles})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Subreddit Predictor Class\n",
    "\n",
    "This will have as attributes the Feature Vectorizers and the Classifiers, which themselves are objects of other classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import secrets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.svm import SVC\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "user_agent = \"Subreddit-Predictor 0.1 by /u/IsThisATrollBot\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets.client_ID,\n",
    "    client_secret=secrets.client_secret,\n",
    "    password=secrets.password,\n",
    "    user_agent=user_agent,\n",
    "    username=secrets.username,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because pushshift is down, we are limited to the amount of data we can gather at a time. So we will choose posts from the 10 most popular subreddits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Start with a list of subreddits\n",
    "top_subreddits = ['announcements', 'funny', 'AskReddit', 'dataisbeautiful', 'Awww', 'datascience', 'pics', 'science', 'worldnews', 'videos', 'AmItheAsshole']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Create an empty list to store the posts\n",
    "posts = []\n",
    "\n",
    "# Iterate through the subreddits and get the last 1000 posts from each\n",
    "for sub in top_subreddits:\n",
    "    subreddit_posts = reddit.subreddit(sub).new(limit=1000)\n",
    "    for post in subreddit_posts:\n",
    "        posts.append(post)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Create a list of dictionaries containing the data for each post\n",
    "data = [{'id': post.id, 'title': post.title, 'subreddit': post.subreddit.display_name} for post in posts]\n",
    "\n",
    "# Create a Pandas dataframe from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "test_titles = ['Redditors of Reddit. What is your favorite piece of Reddit history?', 'WIBTA if I stole my younger brothers lunch money?', 'check out this cool video I found', 'asdf', 'cats are dangerous', 'new study shows cats are dangerous', 'reddit cool aita']\n",
    "test_titles = pd.DataFrame({'title':test_titles})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Subreddit Predictor Class\n",
    "\n",
    "This will have as attributes the Feature Vectorizers and the Classifiers, which themselves are objects of other classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class: Subreddit_Predictor\n",
    "\n",
    "Objects of this class contain attributes and Methods that can be broken up into three categories: **Data**, **Collections**, and **Processing**\n",
    "\n",
    "## Data\n",
    "Pandas DataFrames and methods to update and clean the data.\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "| Name      |   Type    |             Description             |\n",
    "|:----------|:---------:|:-----------------------------------|\n",
    "| raw_data  | DataFrame |      The raw unprocessed data       |\n",
    "| full_data | DataFrame |         The processed data          |\n",
    "| X_train   | DataFrame | the X portion of the training data  |\n",
    "| Y_train   | DataFrame | the Y portion of the training data  |\n",
    "| X_test    | DataFrame |   the X portion of the test data    |\n",
    "| Y_test    | DataFrame |   the Y portion of the test data    |\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "| Name (with input/output typing) | Description                                                                                                                               |\n",
    "|---------------------------------| ------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| add_data(df: DataFrame)         | Updates the raw_data attribute                                                                                                            |\n",
    "| ready_data()                    | Cleans the data and does a test train split. <br/> Overwrites the full_data attribute. <br/> Creates the X_train, Y_train, X_test, and Y_test attributes. |\n",
    "\n",
    "## Collections\n",
    "Contains dictionaries of vectorizers, classifiers, and models\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "| Name        | Type                  | Description                                                                                                                                |\n",
    "|:------------|:----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Vectorizers | dict | Dictionary of Vectorizer objects                                                                                                           |\n",
    "| Classifiers | dict | Dictionary of Classifier objects                                                                                                           |\n",
    "| Models      | dict | Dictionary of trained Classifier objects                                                                                                   |\n",
    "| Models_info | dict        | Dictionary containing a description of each model in Models.                                                                               |\n",
    "| Predictions | DataFrame   | A DataFrame with all the titles and actual subreddits in X_test and Y_test <br/> There is a column for each model that has the predictions |\n",
    "| Results | dict | Dictionary of DataFrames for each model. Each row and column is a subreddit. Shows the number of false classifications |\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "| Name (with input/output typing)                                                                                  | Description                                                                                                                                                                                                                                                        |\n",
    "|------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| add_vectorizer(model: Vectorizer)                                                                                | Adds (key = model.name, value = model) to Vectorizers                                                                                                                                                                                                              |\n",
    "| add_classifier(model: Classifier)                                                                                | Adds (key = model.name, value = model) to Classifiers                                                                                                                                                                                                              |\n",
    "| train_model(<br/>modelName: str, <br/>vectorizerName: str, <br/>classifierName: str, <br/>description = '' :str) | Takes vectorizer and classifer from Vectorizers and Classifiers. <br/>Trains the classifier.<br/>Names and adds the trained model to Models.<br/>Adds the description text to Models_info.|\n",
    "| test_model(modelName: str) |  Runs the model against X_test and Y_test. <br/> Updates Predictions and Results |\n",
    "\n",
    "## Processing\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "| Name (with input/output typing)                       | Description                                                                                   |\n",
    "|-------------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| predict(modelName:str, title: str, titles: iter[str]) | Given a model, enter a title or a list/dataframe of titles. Will return the model prediction. |\n",
    "| compare(models: list[str])                            | Creates a bar chart comparing each of the models on each of the subreddits. |                  |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8048\\2974317389.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "type(obj)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Subreddit_Predictor:\n",
    "    \"\"\"\n",
    "    Objects of this class contain the following:\n",
    "\n",
    "    Data - Pandas DataFrames and methods to update and clean the data.\n",
    "    Attributes:\n",
    "        obj.raw_data, full_data, X_train, Y_train, X_test, Y_test\n",
    "    Methods:\n",
    "        obj.add_data(df), ready_data()\n",
    "\n",
    "    Containers - Dictionaries which contain other objects. The key to each dictionary is always the name.\n",
    "    Attributes:\n",
    "        obj.Vectorizers, Classifiers, Models, Models_info\n",
    "    Methods:\n",
    "        obj.add_vectorizer(model: Vectorizer), obj.add_classifier(model: Classifier), obj.train_model(vectorizerName: str, classifierName: str, modelName: str)\n",
    "\n",
    "    Analyzer - The visual representation of the results of the different models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.raw_data = pd.DataFrame({'id':[], 'title':[], 'subreddit':[]})\n",
    "        self.subreddits = []\n",
    "        self.data = pd.DataFrame({'id':[], 'title':[], 'subreddit':[]})\n",
    "        self.Feature_Vectors = {}\n",
    "        self.Embedding = {}\n",
    "        self.Title_Vectorizers = {}\n",
    "        self.Classifiers = {}\n",
    "        self.Models = {}\n",
    "        self.Models_info = {}\n",
    "\n",
    "    def add_data(self, df):\n",
    "        \"\"\"df is a pandas DataFrame with columns={'title':[], 'subreddit':[]}. It will be merged with the existing raw_data\"\"\"\n",
    "        self.raw_data = pd.concat([self.raw_data, df]).drop_duplicates(subset='id')\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Cleans the data in raw_data and updates self.data\"\"\"\n",
    "\n",
    "        df = self.raw_data\n",
    "\n",
    "        # Remove all non-alpha-numeric characters\n",
    "        df['title'] = df['title'].str.replace(r'[^a-zA-Z0-9 ]', '', regex = True)\n",
    "\n",
    "        # Make all the text lowercase\n",
    "        df['title'] = df['title'].str.lower()\n",
    "\n",
    "        # Remove empty rows\n",
    "        df['title'] = df['title'].str.strip()\n",
    "        filter = df['title'] == ''\n",
    "        df = df.drop(df[filter].index)\n",
    "\n",
    "        # Store it as\n",
    "        self.data = df\n",
    "\n",
    "        #update the subreddits attribute\n",
    "        self.subreddits = self.data['subreddit'].unique().tolist()\n",
    "\n",
    "    def ready_data(self, test_size = .2, seed = 42):\n",
    "        \"\"\"Splits and encodes the data. Saves is in X_train, Y_train, X_test, Y_test.\"\"\"\n",
    "\n",
    "        # Change the index\n",
    "        self.data = self.data.set_index('id')\n",
    "\n",
    "        # Encode the subreddits\n",
    "        self._le = LabelEncoder()\n",
    "        self.data['subreddit_num'] = self._le.fit_transform(self.data['subreddit'])\n",
    "\n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.data['title'], self._le.fit_transform(self.data['subreddit']), test_size=test_size, random_state = seed)\n",
    "\n",
    "    def add_title_vectorizer(self, title_vectorizer):\n",
    "        \"\"\"This is how we add a title_vectorizer to our collection\"\"\"\n",
    "        title_vectorizer.train(self.X_train)\n",
    "        self.Title_Vectorizers[title_vectorizer.featureName] = title_vectorizer\n",
    "        self.Feature_Vectors[title_vectorizer.featureName] = title_vectorizer.vectorize(self.X_train)\n",
    "\n",
    "    def add_classifier(self, classifier):\n",
    "        \"\"\"We add the classifier to our collection, self.Classifiers\"\"\"\n",
    "        self.Classifiers[classifier.classifierName] = classifier\n",
    "\n",
    "    def train_model(self, modelName, featureName, classifierName, description = ''):\n",
    "        \"\"\"\n",
    "        :param modelName: The name of this model\n",
    "        :param featureName: Which feature vectors are we using?\n",
    "        :param classifierName: Which classifier are we using?\n",
    "        :param description: Write a short discription of the model (optional).\n",
    "        :return: Adds a trained object of the classifier class to self.Models\n",
    "        \"\"\"\n",
    "\n",
    "        self.Models_info[modelName] = {'featureName':featureName, 'classifierName':classifierName, 'description':description}\n",
    "\n",
    "        X_train = self.Feature_Vectors[featureName]\n",
    "        Y_train = self.Y_train\n",
    "        classifier = self.Classifiers[classifierName]\n",
    "        classifier.train(X_train, Y_train)\n",
    "\n",
    "        self.Models[modelName] = classifier\n",
    "\n",
    "\n",
    "    def predictions(self, modelName, titles):\n",
    "        \"\"\"\n",
    "        :param modelName: Which model are we using?\n",
    "        :param titles: A list or series of titles\n",
    "        :return: A data frame of 'title' and 'prediction'\n",
    "        \"\"\"\n",
    "\n",
    "        model = self.Models[modelName]\n",
    "\n",
    "        featureName = self.Models_info[modelName]['featureName']\n",
    "        vectorizer = self.Title_Vectorizers[featureName]\n",
    "\n",
    "        title_vectors = vectorizer.vectorize(titles)\n",
    "\n",
    "        df = model.predict(title_vectors)\n",
    "        #df['prediction'] = self._le.inverse_transform(df['prediction'])\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_features(self, featureName):\n",
    "        \"\"\"Generates the features using the different methods we have created\"\"\"\n",
    "\n",
    "        if featureName == 'BoW':\n",
    "            self.Embedding['BoW'] = CountVectorizer()\n",
    "            self.Features['BoW'] = self.Embedding['BoW'].fit_transform(self.X_train)\n",
    "\n",
    "        if featureName == 'D2V':\n",
    "\n",
    "            # Create a list of TaggedDocument objects from the titles\n",
    "            X_train_tagged = self.X_train.tolist()\n",
    "            X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "            X_test_tagged = self.X_test.tolist()\n",
    "            X_test_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_test_tagged)]\n",
    "\n",
    "            model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "            model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "            # Train the model\n",
    "            model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "            # Get the vectorized titles from the doc2vec model\n",
    "            vectors = [model_dbow.infer_vector(title.split()) for title in X_train.tolist()]\n",
    "\n",
    "            # Add the vectors to the dataframe as a new column\n",
    "            df_new = pd.DataFrame({'title':X_train, 'vector': vectors})\n",
    "            df_new\n",
    "\n",
    "    def vectorize(self, featureName, x):\n",
    "        \"\"\"Turns a sentence or list of sentences into a feature vectors\"\"\"\n",
    "\n",
    "        if type(x) == str: return self.vectorize(featureName, [x])\n",
    "\n",
    "        else:\n",
    "            if featureName == 'BoW':\n",
    "                return self.Embedding['BoW'].transform(x).toarray()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj = Subreddit_Predictor()\n",
    "obj.add_data(df)\n",
    "obj.clean_data()\n",
    "obj.ready_data(test_size=.3, seed=29)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.Models_info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.predictions('BoW+SVM', test_titles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Title Vectorizer Class\n",
    "\n",
    "This will have all of the different vectorizers. All of the different ways to embed titles.\n",
    "A key feature of this class is that there are functions which need to be added later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Title_Vectorizer:\n",
    "    \"\"\"This class is to hold all of the Title Vectorizers, like Bag-of-Words and Doc2Vec. Each vectorizer is a specific object. The class methods all have the same input/output.\"\"\"\n",
    "    def __init__(self, featureName):\n",
    "        self.featureName = featureName\n",
    "        self.description = \"Description goes here\"\n",
    "\n",
    "    def train(self, X_train):\n",
    "        \"\"\"Inputs the training data. Creates the self.model\"\"\"\n",
    "\n",
    "        self.model = self._train(X_train)\n",
    "\n",
    "    def _train(self, X_train):\n",
    "        \"\"\"Just a place holder for the actual function\"\"\"\n",
    "        pass\n",
    "\n",
    "    def vectorize(self, df_titles):\n",
    "        \"\"\"Given a data frame or series with only titles, will return a df of all of the features, indexed by id. The actual function will be added to each object.\"\"\"\n",
    "\n",
    "        return self._vectorize(df_titles, self.model)\n",
    "\n",
    "    def _vectorize(self, df_titles, model):\n",
    "        \"\"\"Just a place holder for the actual function.\"\"\"\n",
    "        pass\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Bag-of-Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model = Title_Vectorizer('BoW')\n",
    "\n",
    "def _BoW_vectorize(df_titles, model):\n",
    "    \"\"\"I think I need to drop every word that's not in the vocabulary.\"\"\"\n",
    "\n",
    "    if type(df_titles) == pd.core.frame.DataFrame:\n",
    "        titles = df_titles['title']\n",
    "    else:\n",
    "        titles = df_titles\n",
    "\n",
    "    vocab = model.vocabulary_\n",
    "\n",
    "    titles = titles.apply(lambda s: ' '.join(set(s.split()).intersection(vocab)))\n",
    "    temp = model.transform(titles)\n",
    "    temp = temp.toarray()\n",
    "    temp = pd.DataFrame(temp)\n",
    "    temp['id'] =df_titles.index\n",
    "    temp = temp.set_index('id')\n",
    "    return temp\n",
    "\n",
    "def _BoW_train(X_train):\n",
    "    model = CountVectorizer()\n",
    "    model.fit_transform(X_train)\n",
    "    return model\n",
    "\n",
    "BoW_model._vectorize = _BoW_vectorize\n",
    "BoW_model._train = _BoW_train\n",
    "\n",
    "obj.add_title_vectorizer(BoW_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model.vectorize(test_titles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Doc2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D2V_model = Title_Vectorizer('D2V')\n",
    "#D2V_model.params = {'dm':0, 'vector_size':300, 'negative':5, 'hs':0, 'min_count':2, 'sample':0, 'epochs':100}\n",
    "\n",
    "def _D2V_train(X_train):\n",
    "\n",
    "    X_train_tagged = X_train.tolist()\n",
    "    X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)\n",
    "    model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "    # Train the model\n",
    "    model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "    return model_dbow\n",
    "\n",
    "def _D2V_vectorize(df_titles, model):\n",
    "\n",
    "    vectors = [model.infer_vector(titl.split()) for titl in df_titles.tolist()]\n",
    "    df_new = pd.DataFrame({'title':df_titles, 'vector': vectors})\n",
    "    df_new =df_new['vector'].apply(lambda x: pd.Series(x))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "D2V_model._train = _D2V_train\n",
    "D2V_model._vectorize = _D2V_vectorize\n",
    "\n",
    "obj.add_title_vectorizer(D2V_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers\n",
    "\n",
    "This is the class that holds the classifiers, like XGBoost and Support Vector Machines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class classifier:\n",
    "    \"\"\"This is the class the holds the classifiers\"\"\"\n",
    "\n",
    "    def __init__(self, classifierName):\n",
    "        self.classifierName = classifierName\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        \"\"\"Input the X and Y training data. Then update the model\"\"\"\n",
    "\n",
    "        self.model = self._train(X_train, Y_train)\n",
    "\n",
    "    def _train(self, X_train, Y_train):\n",
    "        \"\"\"Where the real function is stored\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, title_vectors):\n",
    "        \"\"\"\n",
    "        :param title_vectors: A pandas dataframe of the vectorized titles\n",
    "        :return: A pandas series with the predictions\n",
    "        \"\"\"\n",
    "\n",
    "        return self._predict(title_vectors, self.model)\n",
    "\n",
    "    def _predict(self, titles, model):\n",
    "        \"\"\"where the actual function is stored\"\"\"\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SVM_model = classifier('SVM')\n",
    "\n",
    "def _SVM_train(X_train, Y_train):\n",
    "    model = SVC()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def _SVM_predict(title_vectors, model):\n",
    "    \"\"\"enter a list or series or data frame of titles. Outputs prediction in a dataframe\"\"\"\n",
    "\n",
    "    df = model.predict(title_vectors)\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "SVM_model._train = _SVM_train\n",
    "SVM_model._predict = _SVM_predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.add_classifier(SVM_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.train_model('BoW+SVM', 'BoW', 'SVM', description= 'Just a quick test')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.predictions('BoW+SVM', obj.X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Test the model on some new data\n",
    "new_titles = ['Redditors of Reddit. What is your favorite piece of Reddit history?', 'WIBTA if I stole my younger brothers lunch money?', 'check out this cool video I found', 'asdf', 'cats are dangerous', 'new study shows cats are dangerous']\n",
    "new_vectors = Embedding[featureName].transform(new_titles)\n",
    "\n",
    "new_predictions = Models[(featureName, classifierName)].predict(new_vectors)\n",
    "\n",
    "output = pd.DataFrame({'title': new_titles, 'Prediction':new_predictions})\n",
    "output['Prediction'] = le.inverse_transform(output['Prediction'])\n",
    "output\n",
    "\n",
    "\n",
    "\n",
    "Models[(featureName, classifierName)] = SVC()\n",
    "Models[(featureName, classifierName)].fit(Features[featureName], Y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D2V_model.vectorize(obj.X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D2V_model.vectorize(obj.X_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "D2V_model.vectorize(test_titles['title'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "           # Create a list of TaggedDocument objects from the titles\n",
    "X_train_tagged = self.X_train.tolist()\n",
    "X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "X_test_tagged = self.X_test.tolist()\n",
    "X_test_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_test_tagged)]\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)\n",
    "model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "# Train the model\n",
    "model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "# Get the vectorized titles from the doc2vec model\n",
    "vectors = [model_dbow.infer_vector(title.split()) for title in X_train.tolist()]\n",
    "\n",
    "# Add the vectors to the dataframe as a new column\n",
    "df_new = pd.DataFrame({'title':X_train, 'vector': vectors})\n",
    "df_new"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model = Title_Vectorizer('BoW')\n",
    "BoW_model._vectorize = _BoW_vectorize\n",
    "BoW_model._train = _BoW_train\n",
    "#BoW_model.train(obj.X_train)\n",
    "#BoW_model.vectorize(obj.X_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#_BoW_train(obj.X_train)\n",
    "BoW_model._train = _BoW_train\n",
    "BoW_model.train(obj.X_train)\n",
    "type(BoW_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(BoW_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model.model.transform(list(test_titles['title'])).toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample pandas series\n",
    "s = pd.Series(['I love dogs', 'I hate cats', 'I like turtles'])\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = ['I', 'love', 'hate', 'like']\n",
    "\n",
    "# Remove words from the sentences that are not in the vocabulary\n",
    "filtered_s = s.apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "# Print the filtered series\n",
    "print(filtered_s)\n",
    "# Remove words from the sentences that are not in the vocabulary\n",
    "filtered_s = s.apply(lambda x: ' '.join(set(x.split()).intersection(vocab)))\n",
    "\n",
    "# Print the filtered series\n",
    "print(filtered_s)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = CountVectorizer()\n",
    "x.fit_transform(obj.X_train)\n",
    "vocab = x.vocabulary_\n",
    "'im' in vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model.vectorize(pd.DataFrame({'title':test_titles}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(x, obj.X_train.index).info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame({'title':obj.X_train, 'vector': x})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj = Subreddit_Predictor()\n",
    "obj.add_data(df)\n",
    "obj.clean_data()\n",
    "obj.ready_data(test_size=.3, seed=29)\n",
    "obj.add_title_vectorizer(BoW_model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.Feature_Vectors['BoW']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    print ('hello',x)\n",
    "\n",
    "obj.fun = foo\n",
    "\n",
    "obj.fun(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the labels to numerical values\n",
    "le = LabelEncoder()\n",
    "df['subreddit_num'] = le.fit_transform(df['subreddit'])\n",
    "\n",
    "df = df.drop(columns=['subreddit'])\n",
    "\n",
    "#df['subreddit'] = le.inverse_transform(df['subreddit_num'])\n",
    "\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame({'id':['pg006s'], 'title':[a], 'subreddit':['announcements']}).set_index('id')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df_new, df]).drop_duplicates(keep = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep = 'first')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 2, 3, 3], 'B': [4, 5, 5, 6, 6], 'C': [7, 8, 8, 9, 9]})\n",
    "\n",
    "# Find duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "# Print the duplicate rows\n",
    "print(duplicate_rows)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Subreddit_Predictor:\n",
    "    \"\"\"\n",
    "    Objects of this class contain the following:\n",
    "\n",
    "    Data - Pandas DataFrames and methods to update and clean the data.\n",
    "    Attributes:\n",
    "        obj.raw_data, full_data, X_train, Y_train, X_test, Y_test\n",
    "    Methods:\n",
    "        obj.add_data(df), ready_data()\n",
    "\n",
    "    Containers - Dictionaries which contain other objects. The key to each dictionary is always the name.\n",
    "    Attributes:\n",
    "        obj.Vectorizers, Classifiers, Models, Models_info\n",
    "    Methods:\n",
    "        obj.add_vectorizer(model: Vectorizer), obj.add_classifier(model: Classifier), obj.train_model(vectorizerName: str, classifierName: str, modelName: str)\n",
    "\n",
    "    Analyzer - The visual representation of the results of the different models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.raw_data = pd.DataFrame({'id':[], 'title':[], 'subreddit':[]})\n",
    "        self.subreddits = []\n",
    "        self.data = pd.DataFrame({'id':[], 'title':[], 'subreddit':[]})\n",
    "        self.Feature_Vectors = {}\n",
    "        self.Embedding = {}\n",
    "        self.Title_Vectorizers = {}\n",
    "        self.Classifiers = {}\n",
    "        self.Models = {}\n",
    "        self.Models_info = {}\n",
    "\n",
    "    def add_data(self, df):\n",
    "        \"\"\"df is a pandas DataFrame with columns={'title':[], 'subreddit':[]}. It will be merged with the existing raw_data\"\"\"\n",
    "        self.raw_data = pd.concat([self.raw_data, df]).drop_duplicates(subset='id')\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Cleans the data in raw_data and updates self.data\"\"\"\n",
    "\n",
    "        df = self.raw_data\n",
    "\n",
    "        # Remove all non-alpha-numeric characters\n",
    "        df['title'] = df['title'].str.replace(r'[^a-zA-Z0-9 ]', '', regex = True)\n",
    "\n",
    "        # Make all the text lowercase\n",
    "        df['title'] = df['title'].str.lower()\n",
    "\n",
    "        # Remove empty rows\n",
    "        df['title'] = df['title'].str.strip()\n",
    "        filter = df['title'] == ''\n",
    "        df = df.drop(df[filter].index)\n",
    "\n",
    "        # Store it as\n",
    "        self.data = df\n",
    "\n",
    "        #update the subreddits attribute\n",
    "        self.subreddits = self.data['subreddit'].unique().tolist()\n",
    "\n",
    "    def ready_data(self, test_size = .2, seed = 42):\n",
    "        \"\"\"Splits and encodes the data. Saves is in X_train, Y_train, X_test, Y_test.\"\"\"\n",
    "\n",
    "        # Change the index\n",
    "        self.data = self.data.set_index('id')\n",
    "\n",
    "        # Encode the subreddits\n",
    "        self._le = LabelEncoder()\n",
    "        self.data['subreddit_num'] = self._le.fit_transform(self.data['subreddit'])\n",
    "\n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.data['title'], self._le.fit_transform(self.data['subreddit']), test_size=test_size, random_state = seed)\n",
    "\n",
    "    def add_title_vectorizer(self, title_vectorizer):\n",
    "        \"\"\"This is how we add a title_vectorizer to our collection\"\"\"\n",
    "        title_vectorizer.train(self.X_train)\n",
    "        self.Title_Vectorizers[title_vectorizer.featureName] = title_vectorizer\n",
    "        self.Feature_Vectors[title_vectorizer.featureName] = title_vectorizer.vectorize(self.X_train)\n",
    "\n",
    "    def add_classifier(self, classifier):\n",
    "        \"\"\"We add the classifier to our collection, self.Classifiers\"\"\"\n",
    "        self.Classifiers[classifier.classifierName] = classifier\n",
    "\n",
    "    def train_model(self, modelName, featureName, classifierName, description = ''):\n",
    "        \"\"\"\n",
    "        :param modelName: The name of this model\n",
    "        :param featureName: Which feature vectors are we using?\n",
    "        :param classifierName: Which classifier are we using?\n",
    "        :param description: Write a short discription of the model (optional).\n",
    "        :return: Adds a trained object of the classifier class to self.Models\n",
    "        \"\"\"\n",
    "\n",
    "        self.Models_info[modelName] = {'featureName':featureName, 'classifierName':classifierName, 'description':description}\n",
    "\n",
    "        X_train = self.Feature_Vectors[featureName]\n",
    "        Y_train = self.Y_train\n",
    "        classifier = self.Classifiers[classifierName]\n",
    "        classifier.train(X_train, Y_train)\n",
    "\n",
    "        self.Models[modelName] = classifier\n",
    "\n",
    "\n",
    "    def predictions(self, modelName, titles):\n",
    "        \"\"\"\n",
    "        :param modelName: Which model are we using?\n",
    "        :param titles: A list or series of titles\n",
    "        :return: A data frame of 'title' and 'prediction'\n",
    "        \"\"\"\n",
    "\n",
    "        model = self.Models[modelName]\n",
    "\n",
    "        featureName = self.Models_info[modelName]['featureName']\n",
    "        vectorizer = self.Title_Vectorizers[featureName]\n",
    "\n",
    "        title_vectors = vectorizer.vectorize(titles)\n",
    "\n",
    "        df = model.predict(title_vectors)\n",
    "        #df['prediction'] = self._le.inverse_transform(df['prediction'])\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_features(self, featureName):\n",
    "        \"\"\"Generates the features using the different methods we have created\"\"\"\n",
    "\n",
    "        if featureName == 'BoW':\n",
    "            self.Embedding['BoW'] = CountVectorizer()\n",
    "            self.Features['BoW'] = self.Embedding['BoW'].fit_transform(self.X_train)\n",
    "\n",
    "        if featureName == 'D2V':\n",
    "\n",
    "            # Create a list of TaggedDocument objects from the titles\n",
    "            X_train_tagged = self.X_train.tolist()\n",
    "            X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "            X_test_tagged = self.X_test.tolist()\n",
    "            X_test_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_test_tagged)]\n",
    "\n",
    "            model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "            model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "            # Train the model\n",
    "            model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "            # Get the vectorized titles from the doc2vec model\n",
    "            vectors = [model_dbow.infer_vector(title.split()) for title in X_train.tolist()]\n",
    "\n",
    "            # Add the vectors to the dataframe as a new column\n",
    "            df_new = pd.DataFrame({'title':X_train, 'vector': vectors})\n",
    "            df_new\n",
    "\n",
    "    def vectorize(self, featureName, x):\n",
    "        \"\"\"Turns a sentence or list of sentences into a feature vectors\"\"\"\n",
    "\n",
    "        if type(x) == str: return self.vectorize(featureName, [x])\n",
    "\n",
    "        else:\n",
    "            if featureName == 'BoW':\n",
    "                return self.Embedding['BoW'].transform(x).toarray()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj = Subreddit_Predictor()\n",
    "obj.add_data(df)\n",
    "obj.clean_data()\n",
    "obj.ready_data(test_size=.3, seed=29)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.Models_info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.predictions('BoW+SVM', test_titles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Title Vectorizer Class\n",
    "\n",
    "This will have all of the different vectorizers. All of the different ways to embed titles.\n",
    "A key feature of this class is that there are functions which need to be added later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Title_Vectorizer:\n",
    "    \"\"\"This class is to hold all of the Title Vectorizers, like Bag-of-Words and Doc2Vec. Each vectorizer is a specific object. The class methods all have the same input/output.\"\"\"\n",
    "    def __init__(self, featureName):\n",
    "        self.featureName = featureName\n",
    "        self.description = \"Description goes here\"\n",
    "\n",
    "    def train(self, X_train):\n",
    "        \"\"\"Inputs the training data. Creates the self.model\"\"\"\n",
    "\n",
    "        self.model = self._train(X_train)\n",
    "\n",
    "    def _train(self, X_train):\n",
    "        \"\"\"Just a place holder for the actual function\"\"\"\n",
    "        pass\n",
    "\n",
    "    def vectorize(self, df_titles):\n",
    "        \"\"\"Given a data frame or series with only titles, will return a df of all of the features, indexed by id. The actual function will be added to each object.\"\"\"\n",
    "\n",
    "        return self._vectorize(df_titles, self.model)\n",
    "\n",
    "    def _vectorize(self, df_titles, model):\n",
    "        \"\"\"Just a place holder for the actual function.\"\"\"\n",
    "        pass\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Bag-of-Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model = Title_Vectorizer('BoW')\n",
    "\n",
    "def _BoW_vectorize(df_titles, model):\n",
    "    \"\"\"I think I need to drop every word that's not in the vocabulary.\"\"\"\n",
    "\n",
    "    if type(df_titles) == pd.core.frame.DataFrame:\n",
    "        titles = df_titles['title']\n",
    "    else:\n",
    "        titles = df_titles\n",
    "\n",
    "    vocab = model.vocabulary_\n",
    "\n",
    "    titles = titles.apply(lambda s: ' '.join(set(s.split()).intersection(vocab)))\n",
    "    temp = model.transform(titles)\n",
    "    temp = temp.toarray()\n",
    "    temp = pd.DataFrame(temp)\n",
    "    temp['id'] =df_titles.index\n",
    "    temp = temp.set_index('id')\n",
    "    return temp\n",
    "\n",
    "def _BoW_train(X_train):\n",
    "    model = CountVectorizer()\n",
    "    model.fit_transform(X_train)\n",
    "    return model\n",
    "\n",
    "BoW_model._vectorize = _BoW_vectorize\n",
    "BoW_model._train = _BoW_train\n",
    "\n",
    "obj.add_title_vectorizer(BoW_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model.vectorize(test_titles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Doc2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D2V_model = Title_Vectorizer('D2V')\n",
    "#D2V_model.params = {'dm':0, 'vector_size':300, 'negative':5, 'hs':0, 'min_count':2, 'sample':0, 'epochs':100}\n",
    "\n",
    "def _D2V_train(X_train):\n",
    "\n",
    "    X_train_tagged = X_train.tolist()\n",
    "    X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)\n",
    "    model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "    # Train the model\n",
    "    model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "    return model_dbow\n",
    "\n",
    "def _D2V_vectorize(df_titles, model):\n",
    "\n",
    "    vectors = [model.infer_vector(titl.split()) for titl in df_titles.tolist()]\n",
    "    df_new = pd.DataFrame({'title':df_titles, 'vector': vectors})\n",
    "    df_new =df_new['vector'].apply(lambda x: pd.Series(x))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "D2V_model._train = _D2V_train\n",
    "D2V_model._vectorize = _D2V_vectorize\n",
    "\n",
    "obj.add_title_vectorizer(D2V_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers\n",
    "\n",
    "This is the class that holds the classifiers, like XGBoost and Support Vector Machines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class classifier:\n",
    "    \"\"\"This is the class the holds the classifiers\"\"\"\n",
    "\n",
    "    def __init__(self, classifierName):\n",
    "        self.classifierName = classifierName\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        \"\"\"Input the X and Y training data. Then update the model\"\"\"\n",
    "\n",
    "        self.model = self._train(X_train, Y_train)\n",
    "\n",
    "    def _train(self, X_train, Y_train):\n",
    "        \"\"\"Where the real function is stored\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, title_vectors):\n",
    "        \"\"\"\n",
    "        :param title_vectors: A pandas dataframe of the vectorized titles\n",
    "        :return: A pandas series with the predictions\n",
    "        \"\"\"\n",
    "\n",
    "        return self._predict(title_vectors, self.model)\n",
    "\n",
    "    def _predict(self, titles, model):\n",
    "        \"\"\"where the actual function is stored\"\"\"\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SVM_model = classifier('SVM')\n",
    "\n",
    "def _SVM_train(X_train, Y_train):\n",
    "    model = SVC()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def _SVM_predict(title_vectors, model):\n",
    "    \"\"\"enter a list or series or data frame of titles. Outputs prediction in a dataframe\"\"\"\n",
    "\n",
    "    df = model.predict(title_vectors)\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "SVM_model._train = _SVM_train\n",
    "SVM_model._predict = _SVM_predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.add_classifier(SVM_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.train_model('BoW+SVM', 'BoW', 'SVM', description= 'Just a quick test')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.predictions('BoW+SVM', obj.X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Test the model on some new data\n",
    "new_titles = ['Redditors of Reddit. What is your favorite piece of Reddit history?', 'WIBTA if I stole my younger brothers lunch money?', 'check out this cool video I found', 'asdf', 'cats are dangerous', 'new study shows cats are dangerous']\n",
    "new_vectors = Embedding[featureName].transform(new_titles)\n",
    "\n",
    "new_predictions = Models[(featureName, classifierName)].predict(new_vectors)\n",
    "\n",
    "output = pd.DataFrame({'title': new_titles, 'Prediction':new_predictions})\n",
    "output['Prediction'] = le.inverse_transform(output['Prediction'])\n",
    "output\n",
    "\n",
    "\n",
    "\n",
    "Models[(featureName, classifierName)] = SVC()\n",
    "Models[(featureName, classifierName)].fit(Features[featureName], Y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D2V_model.vectorize(obj.X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D2V_model.vectorize(obj.X_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "D2V_model.vectorize(test_titles['title'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "           # Create a list of TaggedDocument objects from the titles\n",
    "X_train_tagged = self.X_train.tolist()\n",
    "X_train_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_train_tagged)]\n",
    "X_test_tagged = self.X_test.tolist()\n",
    "X_test_tagged = [TaggedDocument(words=title.split(), tags=[str(i)]) for i, title in enumerate(X_test_tagged)]\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)\n",
    "model_dbow.build_vocab(X_train_tagged)\n",
    "\n",
    "# Train the model\n",
    "model_dbow.train(X_train_tagged, total_examples=model_dbow.corpus_count, epochs=100)\n",
    "\n",
    "# Get the vectorized titles from the doc2vec model\n",
    "vectors = [model_dbow.infer_vector(title.split()) for title in X_train.tolist()]\n",
    "\n",
    "# Add the vectors to the dataframe as a new column\n",
    "df_new = pd.DataFrame({'title':X_train, 'vector': vectors})\n",
    "df_new"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model = Title_Vectorizer('BoW')\n",
    "BoW_model._vectorize = _BoW_vectorize\n",
    "BoW_model._train = _BoW_train\n",
    "#BoW_model.train(obj.X_train)\n",
    "#BoW_model.vectorize(obj.X_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#_BoW_train(obj.X_train)\n",
    "BoW_model._train = _BoW_train\n",
    "BoW_model.train(obj.X_train)\n",
    "type(BoW_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(BoW_model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model.model.transform(list(test_titles['title'])).toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample pandas series\n",
    "s = pd.Series(['I love dogs', 'I hate cats', 'I like turtles'])\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = ['I', 'love', 'hate', 'like']\n",
    "\n",
    "# Remove words from the sentences that are not in the vocabulary\n",
    "filtered_s = s.apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "# Print the filtered series\n",
    "print(filtered_s)\n",
    "# Remove words from the sentences that are not in the vocabulary\n",
    "filtered_s = s.apply(lambda x: ' '.join(set(x.split()).intersection(vocab)))\n",
    "\n",
    "# Print the filtered series\n",
    "print(filtered_s)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = CountVectorizer()\n",
    "x.fit_transform(obj.X_train)\n",
    "vocab = x.vocabulary_\n",
    "'im' in vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BoW_model.vectorize(pd.DataFrame({'title':test_titles}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(x, obj.X_train.index).info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame({'title':obj.X_train, 'vector': x})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj = Subreddit_Predictor()\n",
    "obj.add_data(df)\n",
    "obj.clean_data()\n",
    "obj.ready_data(test_size=.3, seed=29)\n",
    "obj.add_title_vectorizer(BoW_model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.Feature_Vectors['BoW']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    print ('hello',x)\n",
    "\n",
    "obj.fun = foo\n",
    "\n",
    "obj.fun(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the labels to numerical values\n",
    "le = LabelEncoder()\n",
    "df['subreddit_num'] = le.fit_transform(df['subreddit'])\n",
    "\n",
    "df = df.drop(columns=['subreddit'])\n",
    "\n",
    "#df['subreddit'] = le.inverse_transform(df['subreddit_num'])\n",
    "\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame({'id':['pg006s'], 'title':[a], 'subreddit':['announcements']}).set_index('id')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df_new, df]).drop_duplicates(keep = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep = 'first')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 2, 3, 3], 'B': [4, 5, 5, 6, 6], 'C': [7, 8, 8, 9, 9]})\n",
    "\n",
    "# Find duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "# Print the duplicate rows\n",
    "print(duplicate_rows)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
